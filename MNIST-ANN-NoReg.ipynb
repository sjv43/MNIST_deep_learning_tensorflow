{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "#X_train = X_train - (np.mean(X_train, 0)+10**-8)[np.newaxis, :]\n",
    "#X_test = X_test - (np.mean(X_train, 0)+10**-8)[np.newaxis, :]\n",
    "\n",
    "X_train_full, y_train_full = X_train, y_train\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now: 20181220141523\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "print(\"Now:\", now)\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 1024\n",
    "n_outputs = 10\n",
    "\n",
    "def reset_graph(seed=43):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "eps = 0.00000001\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.random_normal((n_inputs, n_neurons), mean=0.0, stddev=stddev)\n",
    "        \n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        #W = tf.cast(tf.abs(W) >= eps, W.dtype) * W\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.add(tf.matmul(X, W), b)\n",
    "                \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "        \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "hidden1_W = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden1/kernel:0\")\n",
    "hidden2_W = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden2/kernel:0\")\n",
    "logits_W = tf.get_default_graph().get_tensor_by_name(\"dnn/outputs/kernel:0\")\n",
    "    \n",
    "def calcRegValue(W, type=1):\n",
    "    if type == 1:\n",
    "        wMax = 1.0\n",
    "        gt_eps = tf.minimum(1.0, tf.add(1.0, tf.constant(eps/(wMax-eps))*tf.subtract(tf.abs(W), wMax)))\n",
    "        #gt_eps = 1.0  \n",
    "        #lt_eps = tf.constant((1-eps)/eps)*tf.abs(W)\n",
    "        lt_eps = 0.0\n",
    "        reg_value = tf.cast(tf.abs(W) >= eps, W.dtype)*gt_eps + tf.cast(tf.abs(W) < eps, W.dtype)*lt_eps\n",
    "        return tf.reduce_sum(reg_value)\n",
    "    elif type == 2:\n",
    "        reg_value = tf.cast(tf.abs(W) >= eps, W.dtype)*W\n",
    "        return tf.reduce_sum(reg_value)\n",
    "    elif type == 3:\n",
    "        epsMat = tf.constant(eps, shape=W.get_shape())\n",
    "        return tf.reduce_sum(tf.subtract(1.0, tf.pow(epsMat, tf.pow(W, 2))))\n",
    "    elif type == 4:\n",
    "        epsMat = tf.constant(eps, shape=W.get_shape())\n",
    "        return tf.reduce_sum(tf.subtract(1.0, tf.pow(epsMat, tf.abs(W))))\n",
    "    elif type == 5:\n",
    "        return tf.reduce_sum(tf.abs(W))\n",
    "    elif type == 6:\n",
    "        return tf.reduce_sum(tf.pow(W, 2))\n",
    "    elif type == 7:\n",
    "        gt_eps = 1.0  \n",
    "        epsMat = tf.constant(eps, shape=W.get_shape())\n",
    "        lt_eps = 1.0 - tf.pow(epsMat, tf.abs(W))\n",
    "        reg_value = tf.cast(tf.abs(W) >= np.sqrt(eps), W.dtype)*gt_eps + tf.cast(tf.abs(W) < np.sqrt(eps), W.dtype)*lt_eps\n",
    "        return tf.reduce_sum(reg_value)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    regType = 4\n",
    "    total_regvalue = tf.add_n([calcRegValue(hidden1_W, type=regType), \\\n",
    "                               calcRegValue(hidden2_W, type=regType), \\\n",
    "                               calcRegValue(logits_W, type=regType)])\n",
    "    \n",
    "    total_nzw = tf.cast(tf.add_n([tf.count_nonzero(hidden1_W), tf.count_nonzero(hidden2_W), \\\n",
    "                                  tf.count_nonzero(logits_W)]), tf.float32)\n",
    "    total_nzw_eps = tf.cast(tf.add_n([tf.reduce_sum(tf.cast(abs(hidden1_W) < eps, hidden1_W.dtype)), \\\n",
    "                                      tf.reduce_sum(tf.cast(abs(hidden2_W) < eps, hidden2_W.dtype)), \\\n",
    "                                      tf.reduce_sum(tf.cast(abs(logits_W) < eps, logits_W.dtype))]), tf.int64)\n",
    "    \n",
    "    lambdaParam = tf.constant(0.0000001, name=\"lambdaParam\")\n",
    "    #loss_total = tf.add(tf.reduce_mean(xentropy, name=\"loss_total\"), tf.scalar_mul(lambdaParam, total_regvalue))\n",
    "    loss_total = tf.reduce_mean(xentropy, name=\"loss_total\")\n",
    "    \n",
    "learning_rate = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.ProximalGradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    training_op = optimizer.minimize(loss_total)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "reg_summary = tf.summary.scalar('Count_Reg', total_regvalue)\n",
    "loss_summary = tf.summary.scalar('Loss_Total', loss_total)\n",
    "err_summary = tf.summary.scalar('Class_Err_Perc', (1-accuracy)*100)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation losses: 0.1708, 0.0799, 0.2507\tBest Loss: 0.2507\tAccuracy: 93.36%\n",
      "1\tValidation losses: 0.0800, 0.0800, 0.1601\tBest Loss: 0.1601\tAccuracy: 95.80%\n",
      "2\tValidation losses: 0.0417, 0.0801, 0.1218\tBest Loss: 0.1218\tAccuracy: 96.68%\n",
      "3\tValidation losses: 0.0154, 0.0803, 0.0957\tBest Loss: 0.0957\tAccuracy: 97.34%\n",
      "4\tValidation losses: 0.0063, 0.0804, 0.0867\tBest Loss: 0.0867\tAccuracy: 97.44%\n",
      "5\tValidation losses: -0.0003, 0.0805, 0.0802\tBest Loss: 0.0802\tAccuracy: 97.74%\n",
      "6\tValidation losses: -0.0042, 0.0807, 0.0765\tBest Loss: 0.0765\tAccuracy: 97.66%\n",
      "7\tValidation losses: -0.0032, 0.0808, 0.0776\tBest Loss: 0.0765\tAccuracy: 97.56%\n",
      "8\tValidation losses: -0.0028, 0.0810, 0.0781\tBest Loss: 0.0765\tAccuracy: 97.60%\n",
      "9\tValidation losses: -0.0032, 0.0811, 0.0779\tBest Loss: 0.0765\tAccuracy: 97.82%\n",
      "10\tValidation losses: 0.0491, 0.0812, 0.1303\tBest Loss: 0.0765\tAccuracy: 96.42%\n",
      "11\tValidation losses: -0.0173, 0.0814, 0.0641\tBest Loss: 0.0641\tAccuracy: 98.30%\n",
      "12\tValidation losses: -0.0087, 0.0816, 0.0728\tBest Loss: 0.0641\tAccuracy: 98.30%\n",
      "13\tValidation losses: -0.0135, 0.0817, 0.0683\tBest Loss: 0.0641\tAccuracy: 97.94%\n",
      "14\tValidation losses: -0.0142, 0.0819, 0.0677\tBest Loss: 0.0641\tAccuracy: 98.30%\n",
      "15\tValidation losses: -0.0138, 0.0821, 0.0683\tBest Loss: 0.0641\tAccuracy: 98.18%\n",
      "16\tValidation losses: -0.0050, 0.0822, 0.0773\tBest Loss: 0.0641\tAccuracy: 97.98%\n",
      "17\tValidation losses: -0.0077, 0.0824, 0.0747\tBest Loss: 0.0641\tAccuracy: 98.16%\n",
      "18\tValidation losses: 0.0064, 0.0826, 0.0890\tBest Loss: 0.0641\tAccuracy: 97.64%\n",
      "19\tValidation losses: 0.0098, 0.0828, 0.0926\tBest Loss: 0.0641\tAccuracy: 98.06%\n",
      "20\tValidation losses: 0.0060, 0.0830, 0.0890\tBest Loss: 0.0641\tAccuracy: 97.88%\n",
      "21\tValidation losses: 0.0039, 0.0831, 0.0870\tBest Loss: 0.0641\tAccuracy: 97.98%\n",
      "22\tValidation losses: -0.0131, 0.0832, 0.0700\tBest Loss: 0.0641\tAccuracy: 98.42%\n",
      "23\tValidation losses: -0.0195, 0.0832, 0.0637\tBest Loss: 0.0637\tAccuracy: 98.74%\n",
      "24\tValidation losses: -0.0176, 0.0832, 0.0655\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "25\tValidation losses: -0.0182, 0.0832, 0.0650\tBest Loss: 0.0637\tAccuracy: 98.66%\n",
      "26\tValidation losses: -0.0173, 0.0832, 0.0659\tBest Loss: 0.0637\tAccuracy: 98.68%\n",
      "27\tValidation losses: -0.0165, 0.0832, 0.0667\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "28\tValidation losses: -0.0162, 0.0832, 0.0670\tBest Loss: 0.0637\tAccuracy: 98.68%\n",
      "29\tValidation losses: -0.0158, 0.0832, 0.0674\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "30\tValidation losses: -0.0154, 0.0832, 0.0678\tBest Loss: 0.0637\tAccuracy: 98.68%\n",
      "31\tValidation losses: -0.0153, 0.0832, 0.0679\tBest Loss: 0.0637\tAccuracy: 98.70%\n",
      "32\tValidation losses: -0.0151, 0.0832, 0.0681\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "33\tValidation losses: -0.0149, 0.0832, 0.0683\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "34\tValidation losses: -0.0147, 0.0832, 0.0685\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "35\tValidation losses: -0.0146, 0.0832, 0.0687\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "36\tValidation losses: -0.0145, 0.0832, 0.0687\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "37\tValidation losses: -0.0144, 0.0832, 0.0688\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "38\tValidation losses: -0.0144, 0.0832, 0.0688\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "39\tValidation losses: -0.0144, 0.0832, 0.0689\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "40\tValidation losses: -0.0143, 0.0832, 0.0689\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "41\tValidation losses: -0.0143, 0.0832, 0.0689\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "42\tValidation losses: -0.0143, 0.0832, 0.0689\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "43\tValidation losses: -0.0142, 0.0832, 0.0690\tBest Loss: 0.0637\tAccuracy: 98.72%\n",
      "Early stopping!\n",
      "Total running time:\t 417\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_epochs = 2000\n",
    "batch_size = 40\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    np.random.seed(43)\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    batch_num = -1\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        batch_num = batch_num + 1\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield batch_num, X_batch, y_batch\n",
    "\n",
    "def get_lr_step(epochCounter, start_lr, stepsize, end_lr, lr_iter_period):\n",
    "    lr = start_lr - math.floor(epochCounter/lr_iter_period) * stepsize\n",
    "    return max(lr, end_lr)\n",
    "        \n",
    "def get_lr_triangle(epochCounter, lr_min, lr_max, lr_iter_period):\n",
    "    cycle = math.floor(1 + epochCounter/(2*lr_iter_period))\n",
    "    x = abs(epochCounter/lr_iter_period - 2*cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * max(0, (1-x)/math.pow(2, cycle-1))\n",
    "    return lr\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        lr = get_lr_triangle(epoch, lr_min=0.01, lr_max=0.25, lr_iter_period=20)\n",
    "        #lr=0.01\n",
    "        rnd_idx = np.random.permutation(len(X_train))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "            X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, learning_rate: lr})\n",
    "        \n",
    "        loss_val, reg_loss_val, lambda1, acc_val = sess.run([loss_total, total_regvalue, lambdaParam, accuracy], \\\n",
    "                                                   feed_dict={X: X_valid, y: y_valid, learning_rate: lr})\n",
    "        base_loss_val = loss_val - lambda1*reg_loss_val\n",
    "        \n",
    "        if best_loss > loss_val:\n",
    "            save_path = saver.save(sess, \"./model_reuters_countreg_\" + now + \".ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation losses: {:.4f}, {:.4f}, {:.4f}\\tBest Loss: {:.4f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, base_loss_val, lambda1*reg_loss_val, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "end_time = datetime.now()\n",
    "running_time = end_time - start_time\n",
    "print(\"Total running time:\\t\", running_time.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_reuters_countreg_20181220141523.ckpt\n",
      "Batches remaining:     5\n",
      "Batches remaining:     4\n",
      "Batches remaining:     3\n",
      "Batches remaining:     2\n",
      "Batches remaining:     1\n",
      "Test accuracy:    98.52%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model_reuters_countreg_\" + now + \".ckpt\") # or better, use save_path\n",
    "    #saver.restore(sess, \"./model_reuters_countreg_20181216211003.ckpt\") # or better, use save_path\n",
    "    n_batches = len(X_test) // 2000\n",
    "    batch_remaining = n_batches\n",
    "    correct_count = 0.0\n",
    "    for batch_idx in np.array_split(range(len(X_test)), n_batches):\n",
    "        correct_count = correct_count + len(batch_idx)*accuracy.eval(feed_dict={X: X_test[batch_idx], y: y_test[batch_idx]})\n",
    "        print(\"Batches remaining:    \", batch_remaining)\n",
    "        batch_remaining = batch_remaining - 1\n",
    "print(\"Test accuracy:    {:.2f}%\".format(100*correct_count/len(X_test)))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
